{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1706ece4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "keras-unet init: TF version is >= 2.0.0 - using `tf.keras` instead of `Keras`\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import DLlib as dl\n",
    "import pylib as py\n",
    "import tf2lib as tl\n",
    "import tf2gan as gan\n",
    "import wflib as wf\n",
    "\n",
    "import data\n",
    "from keras_unet.models import custom_unet\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import tqdm\n",
    "import h5py\n",
    "import xlsxwriter\n",
    "from skimage.metrics import structural_similarity\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b05fad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir = 'output/TEaug-002'\n",
    "out_vars = 'PM'\n",
    "n_echoes = 6\n",
    "G_model = 'encod-decod'\n",
    "te_input = True\n",
    "bayesian = False\n",
    "n_filters = 72\n",
    "batch_size = 1\n",
    "D1_SelfAttention = False\n",
    "D2_SelfAttention = True\n",
    "D3_SelfAttention = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02fef358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. Elements- DS1: 1\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "############### DIRECTORIES AND FILENAMES ##################\n",
    "############################################################\n",
    "dataset_dir = '../MATLAB/waterFatSignalPhantom/data_out/'\n",
    "dataset_hdf5_1 = 'phantom.hdf5'\n",
    "\n",
    "############################################################\n",
    "################### LOAD DATASET 1 #########################\n",
    "############################################################\n",
    "if G_model == 'complex':\n",
    "    ech_idx = n_echoes\n",
    "else:\n",
    "    ech_idx = n_echoes * 2\n",
    "acqs_1, out_maps_1 = data.load_hdf5(dataset_dir,dataset_hdf5_1, ech_idx, complex_data=(G_model=='complex'))\n",
    "\n",
    "print('Num. Elements- DS1:', len(acqs_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ffd7b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquisition Dimensions: 192 192\n",
      "Echoes: 6\n",
      "Output Maps: 6\n",
      "Testing input shape: (1, 192, 192, 12)\n",
      "Testing output shape: (1, 192, 192, 6)\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "################# DATASET PARTITIONS #######################\n",
    "############################################################\n",
    "\n",
    "testX   = acqs_1\n",
    "\n",
    "testY   = out_maps_1\n",
    "\n",
    "# Overall dataset statistics\n",
    "len_dataset,hgt,wdt,d_ech = np.shape(testX)\n",
    "_,_,_,n_out = np.shape(testY)\n",
    "echoes = int(d_ech/2)\n",
    "r2_sc,fm_sc = 200,300\n",
    "\n",
    "print('Acquisition Dimensions:', hgt,wdt)\n",
    "print('Echoes:',echoes)\n",
    "print('Output Maps:',n_out)\n",
    "\n",
    "# Input and output dimensions (testing data)\n",
    "print('Testing input shape:',testX.shape)\n",
    "print('Testing output shape:',testY.shape)\n",
    "\n",
    "A_B_dataset_test = tf.data.Dataset.from_tensor_slices((testX,testY))\n",
    "A_B_dataset_test.batch(batch_size)\n",
    "test_iter = cycle(A_B_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86041f5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for output/TEaug-002\\checkpoints\\ckpt-49",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py:95\u001b[0m, in \u001b[0;36mNewCheckpointReader\u001b[1;34m(filepattern)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepattern\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# issue with throwing python exceptions from C++.\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for output/TEaug-002\\checkpoints\\ckpt-49",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 47>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m(\u001b[38;5;167;01mNameError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized Generator Architecture\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# restore\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[43mtl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mG_A2B\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mG_A2B\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpoints\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\OT-CycleGAN\\tf2lib\\utils\\utils.py:17\u001b[0m, in \u001b[0;36mCheckpoint.restore\u001b[1;34m(self, save_path)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrestore\u001b[39m(\u001b[38;5;28mself\u001b[39m, save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     16\u001b[0m     save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanager\u001b[38;5;241m.\u001b[39mlatest_checkpoint \u001b[38;5;28;01mif\u001b[39;00m save_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m save_path\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\util.py:2118\u001b[0m, in \u001b[0;36mCheckpoint.restore\u001b[1;34m(self, save_path, options)\u001b[0m\n\u001b[0;32m   2037\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrestore\u001b[39m(\u001b[38;5;28mself\u001b[39m, save_path, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2038\u001b[0m   \u001b[38;5;124;03m\"\"\"Restore a training checkpoint.\u001b[39;00m\n\u001b[0;32m   2039\u001b[0m \n\u001b[0;32m   2040\u001b[0m \u001b[38;5;124;03m  Restores this `Checkpoint` and any objects it depends on.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2116\u001b[0m \u001b[38;5;124;03m        (often at program shutdown).\u001b[39;00m\n\u001b[0;32m   2117\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2118\u001b[0m   status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2119\u001b[0m   \u001b[38;5;66;03m# Create the save counter now so it gets initialized with other variables\u001b[39;00m\n\u001b[0;32m   2120\u001b[0m   \u001b[38;5;66;03m# when graph building. Creating it earlier would lead to errors when using,\u001b[39;00m\n\u001b[0;32m   2121\u001b[0m   \u001b[38;5;66;03m# say, train.Saver() to save the model before initializing it.\u001b[39;00m\n\u001b[0;32m   2122\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_create_save_counter()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\util.py:2035\u001b[0m, in \u001b[0;36mCheckpoint.read\u001b[1;34m(self, save_path, options)\u001b[0m\n\u001b[0;32m   1997\u001b[0m \u001b[38;5;124;03m\"\"\"Read a training checkpoint written with `write`.\u001b[39;00m\n\u001b[0;32m   1998\u001b[0m \n\u001b[0;32m   1999\u001b[0m \u001b[38;5;124;03mReads this `Checkpoint` and any objects it depends on.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2032\u001b[0m \u001b[38;5;124;03m  status of a checkpoint restoration.  See `restore` for details.\u001b[39;00m\n\u001b[0;32m   2033\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2034\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;129;01mor\u001b[39;00m checkpoint_options\u001b[38;5;241m.\u001b[39mCheckpointOptions()\n\u001b[1;32m-> 2035\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_saver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\util.py:1275\u001b[0m, in \u001b[0;36mTrackableSaver.restore\u001b[1;34m(self, save_path, options)\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1274\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m InitializationOnlyStatus(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_view, ops\u001b[38;5;241m.\u001b[39muid())\n\u001b[1;32m-> 1275\u001b[0m reader \u001b[38;5;241m=\u001b[39m \u001b[43mpy_checkpoint_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNewCheckpointReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1276\u001b[0m graph_building \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m graph_building:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py:99\u001b[0m, in \u001b[0;36mNewCheckpointReader\u001b[1;34m(filepattern)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# issue with throwing python exceptions from C++.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 99\u001b[0m   \u001b[43merror_translator\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py:35\u001b[0m, in \u001b[0;36merror_translator\u001b[1;34m(e)\u001b[0m\n\u001b[0;32m     31\u001b[0m error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot found in checkpoint\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_message \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to find any \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatching files for\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m error_message:\n\u001b[1;32m---> 35\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mNotFoundError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, error_message)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSliced checkpoints are not supported\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_message \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData type \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupported\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m error_message:\n\u001b[0;32m     40\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mUnimplementedError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, error_message)\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for output/TEaug-002\\checkpoints\\ckpt-49"
     ]
    }
   ],
   "source": [
    "# model\n",
    "if G_model == 'multi-decod' or G_model == 'encod-decod':\n",
    "    if out_vars == 'WF-PM':\n",
    "        G_A2B=dl.MDWF_Generator(input_shape=(hgt,wdt,d_ech),\n",
    "                                te_input=te_input,\n",
    "                                filters=n_filters,\n",
    "                                WF_self_attention=D1_SelfAttention,\n",
    "                                R2_self_attention=D2_SelfAttention,\n",
    "                                FM_self_attention=D3_SelfAttention)\n",
    "    else:\n",
    "        G_A2B = dl.PM_Generator(input_shape=(hgt,wdt,d_ech),\n",
    "                                te_input=te_input,\n",
    "                                te_shape=(n_echoes,),\n",
    "                                bayesian=bayesian,\n",
    "                                filters=n_filters,\n",
    "                                R2_self_attention=D1_SelfAttention,\n",
    "                                FM_self_attention=D2_SelfAttention)\n",
    "\n",
    "elif G_model == 'U-Net':\n",
    "    if out_vars == 'WF-PM':\n",
    "        n_out = 4\n",
    "    else:\n",
    "        n_out = 2\n",
    "    G_A2B = custom_unet(input_shape=(hgt,wdt,d_ech),\n",
    "                        num_classes=n_out,\n",
    "                        dropout=0.0,\n",
    "                        use_dropout_on_upsampling=True,\n",
    "                        use_attention=D1_SelfAttention,\n",
    "                        filters=n_filters)\n",
    "\n",
    "elif G_model == 'MEBCRN':\n",
    "    if out_vars == 'WF-PM':\n",
    "        n_out = 4\n",
    "    else:\n",
    "        n_out = 2\n",
    "    G_A2B=dl.MEBCRN(input_shape=(hgt,wdt,d_ech),\n",
    "                    n_outputs=n_out,\n",
    "                    n_res_blocks=5,\n",
    "                    n_downsamplings=2,\n",
    "                    filters=n_filters,\n",
    "                    self_attention=D1_SelfAttention)\n",
    "\n",
    "else:\n",
    "    raise(NameError('Unrecognized Generator Architecture'))\n",
    "\n",
    "# restore\n",
    "tl.Checkpoint(dict(G_A2B=G_A2B), py.join(experiment_dir, 'checkpoints')).restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64618c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced-Ideal model\n",
    "@tf.function\n",
    "def sample_A2B(A,TE=None):\n",
    "    indx_B = tf.concat([tf.zeros_like(A[:,:,:,:4],dtype=tf.int32),\n",
    "                        tf.ones_like(A[:,:,:,:2],dtype=tf.int32)],axis=-1)\n",
    "    indx_B_abs = tf.concat([tf.zeros_like(A[:,:,:,:2],dtype=tf.int32),\n",
    "                            tf.ones_like(A[:,:,:,:2],dtype=tf.int32)],axis=-1)\n",
    "    indx_PM =tf.concat([tf.zeros_like(A[:,:,:,:1],dtype=tf.int32),\n",
    "                        tf.ones_like(A[:,:,:,:1],dtype=tf.int32)],axis=-1)\n",
    "    indx_mv =tf.concat([tf.zeros_like(A[:,:,:,:1],dtype=tf.int32),\n",
    "                        tf.ones_like(A[:,:,:,:1],dtype=tf.int32),\n",
    "                        tf.zeros_like(A[:,:,:,:1],dtype=tf.int32),\n",
    "                        tf.ones_like(A[:,:,:,:1],dtype=tf.int32)],axis=-1)\n",
    "    # Estimate A2B\n",
    "    if out_vars == 'WF':\n",
    "        A2B_WF_abs = G_A2B(A, training=False)\n",
    "        A2B_WF_abs = tf.where(A[:,:,:,:2]!=0.0,A2B_WF_abs,0.0)\n",
    "        A2B_PM = tf.zeros_like(A[:,:,:,:2])\n",
    "        A2B_abs = tf.concat([A2B_WF_abs,A2B_PM],axis=-1)\n",
    "    elif out_vars == 'PM':\n",
    "        if te_input:\n",
    "            A2B_PM = G_A2B([A,TE], training=False)\n",
    "            A2B_std = None\n",
    "        elif bayesian:\n",
    "            A2B_PM, A2B_prob = G_A2B(A, training=True)\n",
    "            A2B_mean,A2B_std = tf.dynamic_partition(A2B_prob,indx_mv,num_partitions=2)\n",
    "            A2B_mean = tf.reshape(A2B_mean,A[:,:,:,:2].shape)\n",
    "            A2B_std = tf.reshape(A2B_std,A[:,:,:,:2].shape)\n",
    "        else:\n",
    "            A2B_PM = G_A2B(A, training=False)\n",
    "            A2B_std = None\n",
    "        A2B_PM = tf.where(A[:,:,:,:2]!=0.0,A2B_PM,0.0)\n",
    "        if G_model=='U-Net' or G_model=='MEBCRN':\n",
    "            A2B_R2, A2B_FM = tf.dynamic_partition(A2B_PM,indx_PM,num_partitions=2)\n",
    "            A2B_R2 = tf.reshape(A2B_R2,A[:,:,:,:1].shape)\n",
    "            A2B_FM = tf.reshape(A2B_FM,A[:,:,:,:1].shape)\n",
    "            A2B_FM = (A2B_FM - 0.5) * 2\n",
    "            A2B_FM = tf.where(A[:,:,:,:1]!=0.0,A2B_FM,0.0)\n",
    "            A2B_PM = tf.concat([A2B_R2,A2B_FM],axis=-1)\n",
    "        A2B_WF, A2B2A = wf.acq_to_acq(A,A2B_PM,te=TE,complex_data=(G_model=='complex'))\n",
    "        A2B_WF_real = A2B_WF[:,:,:,0::2]\n",
    "        A2B_WF_imag = A2B_WF[:,:,:,1::2]\n",
    "        A2B_WF_abs = tf.abs(tf.complex(A2B_WF_real,A2B_WF_imag))\n",
    "        A2B_abs = tf.concat([A2B_WF_abs,A2B_PM],axis=-1)\n",
    "    elif out_vars == 'WF-PM':\n",
    "        if te_input:\n",
    "            A2B_abs = G_A2B([A,TE], training=False)\n",
    "        else:\n",
    "            A2B_abs = G_A2B(A, training=False)\n",
    "        A2B_abs = tf.where(A[:,:,:,:4]!=0.0,A2B_abs,0.0)\n",
    "        A2B_WF_abs,A2B_PM = tf.dynamic_partition(A2B_abs,indx_B_abs,num_partitions=2)\n",
    "        A2B_WF_abs = tf.reshape(A2B_WF_abs,A[:,:,:,:2].shape)\n",
    "        A2B_PM = tf.reshape(A2B_PM,A[:,:,:,:2].shape)\n",
    "        if G_model=='U-Net' or G_model=='MEBCRN':\n",
    "            A2B_R2, A2B_FM = tf.dynamic_partition(A2B_PM,indx_PM,num_partitions=2)\n",
    "            A2B_R2 = tf.reshape(A2B_R2,A[:,:,:,:1].shape)\n",
    "            A2B_FM = tf.reshape(A2B_FM,A[:,:,:,:1].shape)\n",
    "            A2B_FM = (A2B_FM - 0.5) * 2\n",
    "            A2B_FM = tf.where(A[:,:,:,:1]!=0.0,A2B_FM,0.0)\n",
    "            A2B_abs = tf.concat([A2B_WF_abs,A2B_R2,A2B_FM],axis=-1)\n",
    "\n",
    "    return [A2B_abs,A2B2A,A2B_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0247e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B = next(test_iter)\n",
    "A = tf.expand_dims(A,axis=0)\n",
    "B = tf.expand_dims(B,axis=0)\n",
    "if te_input:\n",
    "    te_orig = wf.gen_TEvar(n_echoes,batch_size,orig=True)\n",
    "    A2B,_,_ = sample_A2B(A, TE=te_orig)\n",
    "else:\n",
    "    A2B, A2B2A, A2B_std = sample_A2B(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3c4da3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ((ax1,ax2,ax3,ax4),(ax5,ax6,ax7,ax8))=plt.subplots(figsize=(14, 6),\n",
    "                                                        nrows=2, ncols=4)\n",
    "# Ground truth in the first row\n",
    "w_aux = np.squeeze(A2B[:,:,:,0])\n",
    "W_ok = ax1.imshow(w_aux, cmap='bone',\n",
    "                  interpolation='none', vmin=0, vmax=1)\n",
    "fig.colorbar(W_ok, ax=ax1)\n",
    "ax1.axis('off')\n",
    "\n",
    "f_aux = np.squeeze(A2B[:,:,:,1])\n",
    "F_ok = ax2.imshow(f_aux, cmap='pink',\n",
    "                  interpolation='none', vmin=0, vmax=1)\n",
    "fig.colorbar(F_ok, ax=ax2)\n",
    "ax2.axis('off')\n",
    "\n",
    "r2_aux = np.squeeze(A2B[:,:,:,2])\n",
    "r2_ok = ax3.imshow(r2_aux*r2_sc, cmap='copper',\n",
    "                   interpolation='none', vmin=0, vmax=r2_sc)\n",
    "fig.colorbar(r2_ok, ax=ax3)\n",
    "ax3.axis('off')\n",
    "\n",
    "field_aux = np.squeeze(A2B[:,:,:,3]) \n",
    "field_ok = ax4.imshow(field_aux*fm_sc, cmap='twilight',\n",
    "                      interpolation='none', vmin=-fm_sc/2, vmax=fm_sc/2)\n",
    "fig.colorbar(field_ok, ax=ax4)\n",
    "ax4.axis('off')\n",
    "\n",
    "# Computed maps in the second row\n",
    "wn_aux = np.squeeze(np.abs(tf.complex(B[:,:,:,0],B[:,:,:,1])))\n",
    "W_unet = ax5.imshow(wn_aux, cmap='bone',\n",
    "                    interpolation='none', vmin=0, vmax=1)\n",
    "fig.colorbar(W_unet, ax=ax5)\n",
    "ax5.axis('off')\n",
    "\n",
    "fn_aux = np.squeeze(np.abs(tf.complex(B[:,:,:,2],B[:,:,:,3])))\n",
    "F_unet = ax6.imshow(fn_aux, cmap='pink',\n",
    "                    interpolation='none', vmin=0, vmax=1)\n",
    "fig.colorbar(F_unet, ax=ax6)\n",
    "ax6.axis('off')\n",
    "\n",
    "r2n_aux = np.squeeze(B[:,:,:,4])\n",
    "r2_unet = ax7.imshow(r2n_aux*r2_sc, cmap='copper',\n",
    "                     interpolation='none', vmin=0, vmax=r2_sc)\n",
    "fig.colorbar(r2_unet, ax=ax7)\n",
    "ax7.axis('off')\n",
    "\n",
    "fieldn_aux = np.squeeze(B[:,:,:,5])\n",
    "field_unet = ax8.imshow(fieldn_aux*fm_sc, cmap='twilight',\n",
    "                        interpolation='none', vmin=-fm_sc/2, vmax=fm_sc/2)\n",
    "fig.colorbar(field_unet, ax=ax8)\n",
    "ax8.axis('off')\n",
    "\n",
    "plt.subplots_adjust(top=1,bottom=0,right=1,left=0,hspace=0.1,wspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a5fd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISPLAY ERROR (4 OUTPUTS)\n",
    "fig_err, (ix1,ix2,ix3,ix4) = plt.subplots(figsize=(14, 3), nrows=1, ncols=4)\n",
    "\n",
    "# Ground truth in the first row\n",
    "w_err = np.abs(w_aux-wn_aux)\n",
    "Wn_err = ix1.imshow(w_err, cmap='gray',\n",
    "                    interpolation='none', vmin=0, vmax=0.5)\n",
    "fig_err.colorbar(Wn_err, ax=ix1)\n",
    "# ix1.set_title('Water Error')\n",
    "ix1.axis('off')\n",
    "f_err = np.abs(f_aux-fn_aux)\n",
    "Fn_err = ix2.imshow(f_err, cmap='gray',\n",
    "                    interpolation='none', vmin=0, vmax=0.5)\n",
    "fig_err.colorbar(Fn_err, ax=ix2)\n",
    "# ix2.set_title('Fat Error')\n",
    "ix2.axis('off')\n",
    "r2_err = np.abs(r2_aux-r2n_aux)*r2_sc\n",
    "r2n_err = ix3.imshow(r2_err, cmap='gray',\n",
    "                     interpolation='none', vmin=0, vmax=r2_sc/2)\n",
    "fig_err.colorbar(r2n_err, ax=ix3)\n",
    "# ix3.set_title('R2* Error')\n",
    "ix3.axis('off')\n",
    "field_err = np.abs(field_aux-fieldn_aux)*fm_sc\n",
    "fieldn_err = ix4.imshow(field_err, cmap='gray',\n",
    "                        interpolation='none', vmin=0, vmax=fm_sc/8)\n",
    "fig_err.colorbar(fieldn_err, ax=ix4)\n",
    "# ix4.set_title('Field Error')\n",
    "ix4.axis('off')\n",
    "\n",
    "plt.subplots_adjust(top=1,bottom=0,right=1,left=0,hspace=0.1,wspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204425e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if bayesian:\n",
    "    # DISPLAY STD (4 OUTPUTS)\n",
    "    fig_std, oxs = plt.subplots(figsize=(14, 3), nrows=1, ncols=4)\n",
    "\n",
    "    # STDs in the two subplots\n",
    "    r2_std = np.squeeze(A2B_std[:,:,:,0])\n",
    "    R2n_std = oxs[0].imshow(r2_std, cmap='gray',\n",
    "                        interpolation='none')#, vmin=0, vmax=0.5)\n",
    "    fig_std.colorbar(R2n_std, ax=oxs[0])\n",
    "    oxs[0].axis('off')\n",
    "    fm_std = np.squeeze(A2B_std[:,:,:,1])\n",
    "    FMn_std = oxs[1].imshow(fm_std, cmap='gray',\n",
    "                        interpolation='none')#, vmin=0, vmax=0.5)\n",
    "    fig_std.colorbar(FMn_std, ax=oxs[1])\n",
    "    oxs[1].axis('off')\n",
    "\n",
    "    # STD-weighted A2B2A error\n",
    "    msd = tf.reduce_mean(tf.math.squared_difference(A,A2B2A),axis=-1)\n",
    "    std = tf.reduce_mean(A2B_std,axis=-1)\n",
    "    std = tf.where(std<=0.0,1.0,std)\n",
    "    log_std = tf.math.log(std)\n",
    "    STDw_msd = msd/std\n",
    "    STDw_msd += log_std\n",
    "    LOG_std = oxs[2].imshow(np.squeeze(log_std), cmap='gray',\n",
    "                         interpolation='none')#, vmin=0, vmax=r2_sc/2)\n",
    "    fig_std.colorbar(LOG_std, ax=oxs[2])\n",
    "    oxs[2].axis('off')\n",
    "    DIV_std = oxs[3].imshow(np.squeeze(STDw_msd), cmap='gray',\n",
    "                            interpolation='none', vmin=0, vmax=10)\n",
    "    fig_std.colorbar(DIV_std, ax=oxs[3])\n",
    "    oxs[3].axis('off')\n",
    "\n",
    "    plt.subplots_adjust(top=1,bottom=0,right=1,left=0,hspace=0.1,wspace=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa659c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_w = np.mean(tf.abs(w_aux-wn_aux), axis=(0,1))\n",
    "MAE_f = np.mean(tf.abs(f_aux-fn_aux), axis=(0,1))\n",
    "MAE_r2 = np.mean(tf.abs(r2_aux-r2n_aux), axis=(0,1))\n",
    "MAE_fm = np.mean(tf.abs(field_aux-fieldn_aux), axis=(0,1))\n",
    "print('MAEs:',np.round([MAE_w,MAE_f,MAE_r2,MAE_fm],5))\n",
    "print('R2* MAE [1/s]:',np.round(200*MAE_r2,5))\n",
    "print('Field Map MAE [Hz]:',np.round(300*MAE_fm,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11045380",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ssim = structural_similarity(w_aux,wn_aux,multichannel=False)\n",
    "f_ssim = structural_similarity(f_aux,fn_aux,multichannel=False)\n",
    "r2_ssim = structural_similarity(r2_aux,r2n_aux,multichannel=False)\n",
    "fm_ssim = structural_similarity(field_aux,fieldn_aux,multichannel=False)\n",
    "print('SSIMs:',np.round([w_ssim,f_ssim,r2_ssim,fm_ssim],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1a5f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground-Truth PDFF\n",
    "PDFF_res = f_aux/(w_aux+f_aux)\n",
    "PDFF_res[np.isnan(PDFF_res)] = 0.0\n",
    "\n",
    "# Model Measured PDFF\n",
    "PDFF_gt = fn_aux/(wn_aux+fn_aux)\n",
    "PDFF_gt[np.isnan(PDFF_gt)] = 0.0\n",
    "\n",
    "# Segmentation rectangle (ref. values for test data NÂ°105)\n",
    "verts_list = {1:   (69,109),#ok\n",
    "              2:   (61,89),#ok\n",
    "              3:   (69,68),#ok\n",
    "              4:   (90,60),#ok\n",
    "              5:  (110,68),#ok\n",
    "              6:  (119,89),#ok\n",
    "              7:  (110,109)}#ok\n",
    "\n",
    "left_x,sup_y = verts_list[7]\n",
    "rect_hgt,rect_wdt = 13,13\n",
    "rect_gt = patches.Rectangle((left_x,sup_y),rect_wdt,rect_hgt,\n",
    "                            linewidth=1,edgecolor='y',facecolor='none')\n",
    "rect_res = patches.Rectangle((left_x,sup_y),rect_wdt,rect_hgt,\n",
    "                             linewidth=1,edgecolor='r',facecolor='none')\n",
    "\n",
    "fig_pdff, (ex1,ex2) = plt.subplots(figsize=(10, 4), nrows=1, ncols=2)\n",
    "\n",
    "disp_pdff_ok = ex1.imshow(PDFF_gt, cmap='bone',\n",
    "                          interpolation='none', vmin=0, vmax=1)\n",
    "fig_pdff.colorbar(disp_pdff_ok, ax=ex1)\n",
    "# ex1.set_title('PDFF Ground Truth',{'fontsize':15})\n",
    "# ex1.add_patch(rect_gt)\n",
    "ex1.axis('off')\n",
    "\n",
    "disp_pdff_res = ex2.imshow(PDFF_res, cmap='bone',\n",
    "                           interpolation='none', vmin=0, vmax=1)\n",
    "fig_pdff.colorbar(disp_pdff_res, ax=ex2)\n",
    "# ex2.set_title('Resulting PDFF',{'fontsize':15})\n",
    "# ex2.add_patch(rect_res)\n",
    "ex2.axis('off')\n",
    "\n",
    "plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, \n",
    "                    hspace = 0.1, wspace = 0)\n",
    "plt.margins(0,0)\n",
    "plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "plt.gca().yaxis.set_major_locator(plt.NullLocator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36783243",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1,r2 = sup_y,(sup_y+rect_hgt)\n",
    "c1,c2 = left_x,(left_x+rect_wdt)\n",
    "PDFF_gt_crop = PDFF_gt[r1:r2,c1:c2]\n",
    "PDFF_res_crop = PDFF_res[r1:r2,c1:c2]\n",
    "\n",
    "print('Median of PDFF in the segmented region:')\n",
    "print('Ground-Truth:\\t',np.median(PDFF_gt_crop))\n",
    "print('Model result:\\t',np.median(PDFF_res_crop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c3ef2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PDFF_res_all = np.array([])\n",
    "PDFF_gt_all = np.array([])\n",
    "rect_hgt,rect_wdt = 13,13\n",
    "for k in verts_list:\n",
    "    left_x,sup_y = verts_list[k]\n",
    "    r1,r2 = sup_y,(sup_y+rect_hgt)\n",
    "    c1,c2 = left_x,(left_x+rect_wdt)\n",
    "    # PDFF crops - magnitude model results\n",
    "    # w_all = all_test_ans[(k_prev+4):(k-4),r1:r2,c1:c2,0]\n",
    "    # f_all = all_test_ans[(k_prev+4):(k-4),r1:r2,c1:c2,1]\n",
    "    # PDFF crops - complex model results\n",
    "    w_all = A2B[:,r1:r2,c1:c2,0]\n",
    "    f_all = A2B[:,r1:r2,c1:c2,1]\n",
    "    PDFF_res_aux = np.median(f_all/(f_all+w_all),axis=(1,2))\n",
    "    PDFF_res_all = np.concatenate((PDFF_res_all,PDFF_res_aux),axis=0)\n",
    "    # PDFF crops - GT magnitude results\n",
    "    # w_all_gt = testY[(k_prev+4):(k-4):,r1:r2,c1:c2,0]\n",
    "    # f_all_gt = testY[(k_prev+4):(k-4),r1:r2,c1:c2,1]\n",
    "    # PDFF crops - GT complex results\n",
    "    w_all_gt = np.abs(tf.complex(B[:,r1:r2,c1:c2,0],\n",
    "                                 B[:,r1:r2,c1:c2,1]))\n",
    "    f_all_gt = np.abs(tf.complex(B[:,r1:r2,c1:c2,2],\n",
    "                                 B[:,r1:r2,c1:c2,3]))\n",
    "    PDFF_gt_aux = np.median(f_all_gt/(f_all_gt+w_all_gt),axis=(1,2))\n",
    "    PDFF_gt_all = np.concatenate((PDFF_gt_all,PDFF_gt_aux),axis=0)\n",
    "\n",
    "# Compute error\n",
    "PDFF_err_list = PDFF_res_all-PDFF_gt_all\n",
    "print('PDFF estimations:',np.round(PDFF_res_all,5))\n",
    "print('Ground-truth:',np.round(PDFF_gt_all,5))\n",
    "print('Errors:',np.round(PDFF_err_list,5))\n",
    "\n",
    "# PDFF crops mean error\n",
    "PDFF_err_mean = np.mean((PDFF_err_list))\n",
    "PDFF_err_std = np.std((PDFF_err_list))\n",
    "print('Mean PDFF error:',np.round(PDFF_err_mean*100,2),\n",
    "      '+-',np.round(PDFF_err_std*100,2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4e5bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_res_all = np.array([])\n",
    "r2_gt_all = np.array([])\n",
    "for k in verts_list:\n",
    "    left_x,sup_y = verts_list[k]\n",
    "    r1,r2 = sup_y,(sup_y+rect_hgt)\n",
    "    c1,c2 = left_x,(left_x+rect_wdt)\n",
    "    # R2* crops - complex model results\n",
    "    r2_all = A2B[:,r1:r2,c1:c2,2]*r2_sc\n",
    "    r2_res_aux = np.mean(r2_all,axis=(1,2))\n",
    "    r2_res_all = np.concatenate((r2_res_all,r2_res_aux),axis=0)\n",
    "    # R2* crops - GT complex results\n",
    "    r2_all_gt = B[:,r1:r2,c1:c2,4]*r2_sc\n",
    "    r2_gt_aux = np.mean(r2_all_gt,axis=(1,2))\n",
    "    r2_gt_all = np.concatenate((r2_gt_all,r2_gt_aux),axis=0)\n",
    "\n",
    "# Compute error\n",
    "r2_err_list = r2_res_all-r2_gt_all\n",
    "print('R2* estimations:',r2_res_all)\n",
    "print('Ground-truth:',r2_gt_all)\n",
    "print('Errors:\\\\n',r2_err_list)\n",
    "\n",
    "# R2* crops mean error\n",
    "r2_err_mean = np.mean((r2_err_list))\n",
    "r2_err_std = np.std((r2_err_list))\n",
    "print('Mean R2* error:',np.round(r2_err_mean,2),\n",
    "      '+-',np.round(r2_err_std,2),'[1/s]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
