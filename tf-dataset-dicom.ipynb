{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b34a54d0-351b-4c19-9136-f53f7393ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import tf2lib as tl\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nibabel as nib\n",
    "import pydicom\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3994bb5-6155-4ee1-9e5e-026efd291a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dicom_series(folder_path):\n",
    "    \"\"\"\n",
    "    Load DICOM series from a folder (multiple echoes per sample).\n",
    "    \n",
    "    Args:\n",
    "        folder_path: str, path to folder containing DICOM files\n",
    "        num_echoes: int, expected number of echoes (optional, for reshape)\n",
    "        target_shape: tuple (H, W), desired image size (optional, for resize)\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray of shape (num_echoes, H, W)\n",
    "    \"\"\"\n",
    "    dicom_files = sorted([os.path.join(folder_path, f) for f in os.listdir(folder_path)\n",
    "                          if f.endswith(\".dcm\")])\n",
    "    \n",
    "    images = []\n",
    "    n_sl = 0\n",
    "    err_flag = False\n",
    "    for j, f in enumerate(dicom_files):\n",
    "        ds = pydicom.dcmread(f)\n",
    "        img = ds.pixel_array.astype(np.float32)\n",
    "\n",
    "        img_comp = str(ds.get((0x2005, 0x1011), pydicom.DataElement((0x2005, 0x1011), 'DS', 1)).value)\n",
    "        echo_time = float(ds.get((0x0018, 0x0081), pydicom.DataElement((0x0018, 0x0081), 'DS', 1)).value)\n",
    "        echo_num = int(ds.get((0x0018, 0x0086), pydicom.DataElement((0x0018, 0x0086), 'DS', 1)).value)\n",
    "        echo_all = int(ds.get((0x0018, 0x0091), pydicom.DataElement((0x0018, 0x0091), 'DS', 1)).value)\n",
    "        RescaleIntercept = float(ds.get((0x2005, 0x100D), pydicom.DataElement((0x2005, 0x100D), 'DS', 1)).value)\n",
    "        RescaleSlope = float(ds.get((0x2005, 0x100A), pydicom.DataElement((0x2005, 0x100A), 'DS', 1)).value)\n",
    "        # print(j, \"Component: \", img_comp)\n",
    "\n",
    "        resc_img = RescaleSlope*(img-RescaleIntercept)\n",
    "\n",
    "        if img_comp == \"R\" and echo_num == 1:\n",
    "            err_flag = False\n",
    "            echoes = list()\n",
    "\n",
    "        if img_comp == \"R\":\n",
    "            aux_img = np.expand_dims(resc_img,axis=-1)\n",
    "        elif img_comp == \"I\":\n",
    "            aux_img = np.concatenate([aux_img,np.expand_dims(resc_img,axis=-1)], axis=-1)\n",
    "            if aux_img.shape[-1] == 2:\n",
    "                echoes.append(aux_img)\n",
    "            else:\n",
    "                err_flag = True\n",
    "\n",
    "            if echo_all == len(echoes) and (not err_flag):\n",
    "                im_echoes = np.stack(echoes, axis=0)\n",
    "                # Normalization considering max magnitude value\n",
    "                mag_echoes = np.sqrt(np.sum(np.square(im_echoes),axis=-1,keepdims=True))\n",
    "                im_echoes_norm = im_echoes/np.max(mag_echoes)\n",
    "                # Masking considering magnitude threshold\n",
    "                im_echoes_mean = np.mean(mag_echoes/np.max(mag_echoes),axis=0,keepdims=True)\n",
    "                im_echoes_mask = np.repeat(im_echoes_mean>0.05,echo_all,axis=0)\n",
    "                im_echoes_mask = np.repeat(im_echoes_mask,2,axis=-1)\n",
    "                images.append(np.where(im_echoes_mask,im_echoes_norm,0.0))\n",
    "                n_sl += 1\n",
    "\n",
    "    # print(\"No. slices:\", n_sl)\n",
    "    # for i, arr in enumerate(images):\n",
    "        # print(i, arr.shape, arr.dtype)\n",
    "    images = np.stack(images, axis=0)  # (num_echoes, H, W, 2)\n",
    "\n",
    "    # Quantitative maps\n",
    "    # out_maps = np.ones([n_sl,3,images.shape[1],images.shape[2],2], dtype=np.float32)\n",
    "\n",
    "    return images\n",
    "\n",
    "def tf_load_dicom_series(folder_path):\n",
    "    \"\"\"\n",
    "    Wrap load_dicom_series in tf.py_function for use in tf.data.Dataset.\n",
    "    \"\"\"\n",
    "    def _load(path):\n",
    "        path = path.numpy().decode(\"utf-8\")\n",
    "        arr = load_dicom_series(path)\n",
    "        return arr\n",
    "\n",
    "    data = tf.py_function(\n",
    "        func=_load,\n",
    "        inp=[folder_path],\n",
    "        Tout=tf.float32\n",
    "    )\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0905a235-2cfe-485d-8fde-c0f044cd8ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifti_series(folder_path):\n",
    "    dicom_files = sorted([os.path.join(folder_path, f) for f in os.listdir(folder_path)\n",
    "                          if f.endswith(\".nii.gz\")])\n",
    "    avoid_comps = ['imaginary','real','Eq']\n",
    "    dicom_files = sorted([f for f in dicom_files if not any(char in f for char in avoid_comps)])\n",
    "    # print(dicom_files)\n",
    "\n",
    "    nifti_file = dicom_files[0]\n",
    "    fn_noEch = nifti_file.split(\"_e\")[0]\n",
    "\n",
    "    # Load NIfTI image\n",
    "    img = nib.load(nifti_file)\n",
    "    data = img.get_fdata(dtype=np.float32)  # shape: (X, Y, Z, echoes?) or (X, Y, Z)\n",
    "    \n",
    "    # Load JSON sidecar (contains echo times, etc.)\n",
    "    json_file = nifti_file.replace(\".nii.gz\", \".json\")\n",
    "    metadata = {}\n",
    "    if os.path.exists(json_file):\n",
    "        with open(json_file, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "    # Define useful parameters\n",
    "    ne = metadata[\"EchoTrainLength\"]\n",
    "\n",
    "    # Generate multi-echo volume to be filled\n",
    "    V_shape = list(data.shape)\n",
    "    V_shape.insert(2, ne)\n",
    "    V = np.zeros(V_shape+[2], dtype=np.float32)\n",
    "    TE = np.zeros((V_shape[-1],ne), dtype=np.float32)\n",
    "    V_mag_all = np.zeros(V_shape, dtype=np.float32)\n",
    "\n",
    "    for ech in range(ne):\n",
    "        # Magnitude file processing\n",
    "        nifti_file_mag = fn_noEch + '_e' + str(ech+1) + '.nii.gz'\n",
    "        img_mag = nib.load(nifti_file_mag)\n",
    "        V_mag = img_mag.get_fdata(dtype=np.float32)\n",
    "\n",
    "        json_file_mag = nifti_file_mag.replace(\".nii.gz\", \".json\")\n",
    "        metadata_mag = {}\n",
    "        if os.path.exists(json_file_mag):\n",
    "            with open(json_file_mag, \"r\") as f:\n",
    "                metadata_mag = json.load(f)\n",
    "\n",
    "        V_mag_resc = np.array(V_mag) # / float(metadata_mag[\"PhilipsScaleSlope\"]);\n",
    "        if ech == 0:\n",
    "            V_sc = np.max(V_mag_resc)\n",
    "\n",
    "        # Phase file processing\n",
    "        nifti_file_ph = fn_noEch + '_e' + str(ech+1) + '_ph.nii.gz'\n",
    "        img_ph = nib.load(nifti_file_ph)\n",
    "        V_ph = img_ph.get_fdata(dtype=np.float32)\n",
    "\n",
    "        json_file_ph = nifti_file_ph.replace(\".nii.gz\", \".json\")\n",
    "        metadata_ph = {}\n",
    "        if os.path.exists(json_file_ph):\n",
    "            with open(json_file_ph, \"r\") as f:\n",
    "                metadata_ph = json.load(f)\n",
    "\n",
    "        V_ph_resc = np.array(V_ph) # / float(metadata_ph[\"PhilipsScaleSlope\"]);\n",
    "\n",
    "        # Combining into complex volume\n",
    "        if V_mag_resc.shape[2] == V_ph_resc.shape[2]:\n",
    "            V_ech = V_mag_resc * np.exp(1j*V_ph_resc) / V_sc;\n",
    "        else:\n",
    "            V_ech = np.zeros([V.shape[0],V.shape[1],V.shape[3]]);\n",
    "            print('\\tMismatch between mag and phase at echo:',str(ech))\n",
    "\n",
    "        if V.shape[3] == V_ech.shape[2]:\n",
    "            V[:,:,ech,:,0] = np.real(V_ech);\n",
    "            V[:,:,ech,:,1] = np.imag(V_ech);\n",
    "            TE[:,ech] = float(metadata_mag[\"EchoTime\"]);\n",
    "            V_mag_all[:,:,ech,:] = np.abs(V_ech)\n",
    "        else:\n",
    "            print('\\tMismatch between complex array and 1st echo at echo',str(ech))\n",
    "\n",
    "    # Get mask from the mean of all-echoes magnitudes\n",
    "    V_mag_mean = np.mean(V_mag_all,axis=2,keepdims=True)\n",
    "    V_mag_mean = np.expand_dims(np.repeat(V_mag_mean,ne,axis=2),axis=-1)\n",
    "    V_mag_mean = np.repeat(V_mag_mean,2,axis=-1)\n",
    "    V = np.where(V_mag_mean >= 0.05, V, 0.0)\n",
    "    \n",
    "    # Rearrange to obtain the required dimensionality\n",
    "    V = np.transpose(V, axes=[3,2,1,0,4]) # (num_slices, num_echoes, H, W, 2)\n",
    "    V = np.flip(V,axis=2)\n",
    "\n",
    "    return V\n",
    "\n",
    "def tf_load_nifti_series(folder_path):\n",
    "    \"\"\"\n",
    "    Wrap load_nifti_series in tf.py_function for use in tf.data.Dataset.\n",
    "    \"\"\"\n",
    "    def _load(path):\n",
    "        path = path.numpy().decode(\"utf-8\")\n",
    "        arr = load_nifti_series(path)\n",
    "        return arr\n",
    "\n",
    "    data = tf.py_function(\n",
    "        func=_load,\n",
    "        inp=[folder_path],\n",
    "        Tout=tf.float32\n",
    "    )\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f35786f-9697-4a63-9d8a-3df0064324aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[WinError 267] El nombre del directorio no es válido: '../AI-PDFF/data/BVH003\\\\BVH003.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m folders_cse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m folders_mr:\n\u001b[1;32m----> 6\u001b[0m     scan_files \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     cse_scan \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m scan_files \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnifti\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m item]\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cse_scan) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mNotADirectoryError\u001b[0m: [WinError 267] El nombre del directorio no es válido: '../AI-PDFF/data/BVH003\\\\BVH003.zip'"
     ]
    }
   ],
   "source": [
    "root = \"../AI-PDFF/data/\"\n",
    "folders = [os.path.join(root, d) for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))]\n",
    "folders_mr = [os.path.join(f, os.listdir(f)[0]) for i, f in enumerate(folders) if os.path.join(f, os.listdir(f)[0])]\n",
    "folders_cse = list()\n",
    "for f in folders_mr:\n",
    "    scan_files = os.listdir(f)\n",
    "    cse_scan = [item for item in scan_files if \"nifti\" in item]\n",
    "    if len(cse_scan) == 0:\n",
    "        cse_scan.append('')\n",
    "    folders_cse.append(os.path.join(f,cse_scan[0]))\n",
    "\n",
    "folders_cse = [folders_cse[2],folders_cse[10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4bc5f4e-19d7-43e0-a80e-244ac8dedd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../AI-PDFF/data/BVH001\\MR_20180404\n",
      "../AI-PDFF/data/BVH002\\MR_20170705\n",
      "../AI-PDFF/data/BVH005\\MR_20170531\n",
      "../AI-PDFF/data/BVH007\\MR_20180314\n",
      "../AI-PDFF/data/BVH008\\MR_20171218\n",
      "../AI-PDFF/data/BVH009\\MR_20180207\n",
      "../AI-PDFF/data/BVH011\\MR_20180813\n",
      "../AI-PDFF/data/BVH012\\MR_20170315\n",
      "../AI-PDFF/data/BVH013\\MR_20171122\n",
      "../AI-PDFF/data/BVH015\\MR_20170320\n",
      "../AI-PDFF/data/BVH016\\MR_20220823\n",
      "../AI-PDFF/data/BVH017\\MR_20170705\n",
      "../AI-PDFF/data/BVH018\\MR_20180124\n",
      "../AI-PDFF/data/BVH019\\MR_20170331\n",
      "../AI-PDFF/data/BVH020\\MR_20180502\n",
      "../AI-PDFF/data/BVH021\\MR_20180525\n",
      "../AI-PDFF/data/BVH026\\MR_20180608\n",
      "../AI-PDFF/data/BVH027\\MR_20220824\n"
     ]
    }
   ],
   "source": [
    "root = \"../AI-PDFF/data/\"\n",
    "folders = [os.path.join(root, d) for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))]\n",
    "folders_mr = [os.path.join(f, os.listdir(f)[0]) for i, f in enumerate(folders) if os.path.join(f, os.listdir(f)[0])]\n",
    "folders_cse = list()\n",
    "for f in folders_mr:\n",
    "    if os.path.isdir(f):\n",
    "        scan_files = os.listdir(f)\n",
    "        cse_scan = [item for item in scan_files if \"MECSE\" in item]\n",
    "        folders_cse.append(os.path.join(f,cse_scan[0]))\n",
    "num_fold = len(folders_cse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "787798cb-3997-48b5-9344-395abfa6a189",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_B_dataset = tf.data.Dataset.from_tensor_slices(folders_cse[12:15])\n",
    "A_B_dataset = A_B_dataset.map(lambda f: data.tf_load_dicom_series(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63bb3831-edbf-421c-a989-352b8d506b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    }
   ],
   "source": [
    "A_B_dataset = A_B_dataset.unbatch()\n",
    "len_dataset = sum(1 for _ in A_B_dataset)\n",
    "print(len_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "643c0b03-1ce4-4e83-bba9-a37e6094cb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../AI-PDFF/data/BVH020\\\\MR_20180502\\\\MECSE_301_MECSE']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders_cse[14:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74007160-483f-407f-b1f9-04442e672df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(folders_cse):\n",
    "    print(i,x,type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c29ab45-7e13-4982-a801-7abedabe7485",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(folders_cse)\n",
    "dataset = dataset.map(lambda f: tf_load_nifti_series(f))\n",
    "dataset = dataset.unbatch()\n",
    "\n",
    "# Example batching\n",
    "dataset = dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621affa3-b9ed-4520-a1c5-049b13a98879",
   "metadata": {},
   "outputs": [],
   "source": [
    "for A in dataset.skip(10).take(1):\n",
    "    print(A.shape)  # (batch_size, num_echoes, H, W, 2)\n",
    "    # print(B.shape)  # (batch_size, 3, H, W, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0502fd1-45b7-470f-b9fe-0f3a9a69a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(22, 6), nrows=2, ncols=6)\n",
    "\n",
    "# Recon MR images at each echo\n",
    "pha_ech1 = np.squeeze(np.arctan2(A[:,0,:,:,1],A[:,0,:,:,0]))\n",
    "pha_ech2 = np.squeeze(np.arctan2(A[:,1,:,:,1],A[:,1,:,:,0]))\n",
    "pha_ech3 = np.squeeze(np.arctan2(A[:,2,:,:,1],A[:,2,:,:,0]))\n",
    "pha_ech4 = np.squeeze(np.arctan2(A[:,3,:,:,1],A[:,3,:,:,0]))\n",
    "pha_ech5 = np.squeeze(np.arctan2(A[:,4,:,:,1],A[:,4,:,:,0]))\n",
    "pha_ech6 = np.squeeze(np.arctan2(A[:,5,:,:,1],A[:,5,:,:,0]))\n",
    "\n",
    "mag_ech1 = np.squeeze(np.abs(tf.complex(A[:,0,:,:,0],A[:,0,:,:,1])))\n",
    "mag_ech2 = np.squeeze(np.abs(tf.complex(A[:,1,:,:,0],A[:,1,:,:,1])))\n",
    "mag_ech3 = np.squeeze(np.abs(tf.complex(A[:,2,:,:,0],A[:,2,:,:,1])))\n",
    "mag_ech4 = np.squeeze(np.abs(tf.complex(A[:,3,:,:,0],A[:,3,:,:,1])))\n",
    "mag_ech5 = np.squeeze(np.abs(tf.complex(A[:,4,:,:,0],A[:,4,:,:,1])))\n",
    "mag_ech6 = np.squeeze(np.abs(tf.complex(A[:,5,:,:,0],A[:,5,:,:,1])))\n",
    "\n",
    "# Acquisitions in the first row\n",
    "acqm_ech1 = axs[0,0].imshow(mag_ech1, cmap='gray',\n",
    "                      interpolation='none', vmin=0, vmax=1)\n",
    "axs[0,0].set_title('Mag 1st Echo')\n",
    "axs[0,0].axis('off')\n",
    "acqm_ech2 = axs[0,1].imshow(mag_ech2, cmap='gray',\n",
    "                          interpolation='none', vmin=0, vmax=1)\n",
    "axs[0,1].set_title('Mag 2nd Echo')\n",
    "axs[0,1].axis('off')\n",
    "acqm_ech3 = axs[0,2].imshow(mag_ech3, cmap='gray',\n",
    "                          interpolation='none', vmin=0, vmax=1)\n",
    "axs[0,2].set_title('Mag 3rd Echo')\n",
    "axs[0,2].axis('off')\n",
    "acqm_ech4 = axs[0,3].imshow(mag_ech4, cmap='gray',\n",
    "                      interpolation='none', vmin=0, vmax=1)\n",
    "axs[0,3].set_title('Mag 4th Echo')\n",
    "axs[0,3].axis('off')\n",
    "acqm_ech5 = axs[0,4].imshow(mag_ech5, cmap='gray',\n",
    "                          interpolation='none', vmin=0, vmax=1)\n",
    "axs[0,4].set_title('Mag 5th Echo')\n",
    "axs[0,4].axis('off')\n",
    "acqm_ech6 = axs[0,5].imshow(mag_ech6, cmap='gray',\n",
    "                          interpolation='none', vmin=0, vmax=1)\n",
    "axs[0,5].set_title('Mag 6th Echo')\n",
    "axs[0,5].axis('off')\n",
    "\n",
    "\n",
    "acqp_ech1 = axs[1,0].imshow(pha_ech1, cmap='gist_earth',\n",
    "                      interpolation='none', vmin=-np.pi, vmax=np.pi)\n",
    "axs[1,0].set_title('Pha 1st Echo')\n",
    "axs[1,0].axis('off')\n",
    "acqp_ech2 = axs[1,1].imshow(pha_ech2, cmap='gist_earth',\n",
    "                          interpolation='none', vmin=-np.pi, vmax=np.pi)\n",
    "axs[1,1].set_title('Pha 2nd Echo')\n",
    "axs[1,1].axis('off')\n",
    "acqp_ech3 = axs[1,2].imshow(pha_ech3, cmap='gist_earth',\n",
    "                          interpolation='none', vmin=-np.pi, vmax=np.pi)\n",
    "axs[1,2].set_title('Pha 3rd Echo')\n",
    "axs[1,2].axis('off')\n",
    "acqp_ech4 = axs[1,3].imshow(pha_ech4, cmap='gist_earth',\n",
    "                      interpolation='none', vmin=-np.pi, vmax=np.pi)\n",
    "axs[1,3].set_title('Pha 4th Echo')\n",
    "axs[1,3].axis('off')\n",
    "acqp_ech5 = axs[1,4].imshow(pha_ech5, cmap='gist_earth',\n",
    "                          interpolation='none', vmin=-np.pi, vmax=np.pi)\n",
    "axs[1,4].set_title('Pha 5th Echo')\n",
    "axs[1,4].axis('off')\n",
    "acqp_ech6 = axs[1,5].imshow(pha_ech6, cmap='gist_earth',\n",
    "                          interpolation='none', vmin=-np.pi, vmax=np.pi)\n",
    "axs[1,5].set_title('Pha 6th Echo')\n",
    "axs[1,5].axis('off')\n",
    "\n",
    "plt.subplots_adjust(top=1,bottom=0,right=1,left=0,hspace=0.1,wspace=0)\n",
    "tl.make_space_above(axs,topmargin=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f1eb0-a73f-472b-9dd6-f7f202f3f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wflib as wf\n",
    "te = wf.gen_TEvar(12, bs=1, TE_ini_min=0.879e-3, TE_ini_d=None, d_TE_min=0.6623e-3, d_TE_d=None) \n",
    "np.round(te.numpy(),6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
